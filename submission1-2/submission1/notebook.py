# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YWxnUYpvLlU2sSX9KZbRw2O3qaGwqHjx

# Life Expectancy Regression

## Data Loading
"""

import pandas as pd

df = pd.read_csv('data/Life Expectancy Data.csv')

"""Pertama, kita perlu melakukan loading dataset. Kali ini kita menggunakan pd.read_csv karena data yang kita miliki berbentuk CSV."""

df.head()

"""Untuk memastikan data yang kita miliki telah dimuat dengan benar, lakukan pengecekan dengan df.head() untuk menampilkan 5 baris teratas dalam dataframe.

## Analisis distribusi dan noise pada data
"""

df.info()

"""Masing-masing variabel yang ada pada dataset memiliki deskripsi sebagai berikut:

| No | Nama Variabel                     | Deskripsi                                                                 |
|----|-----------------------------------|---------------------------------------------------------------------------|
| 1  | Country                           | Nama negara                                                               |
| 2  | Year                              | Tahun pengamatan (2000–2015)                                              |
| 3  | Status                            | Status perkembangan negara (Developed/Developing)                         |
| 4  | Life expectancy                   | Harapan hidup rata-rata saat lahir                                        |
| 5  | Adult Mortality                   | Angka kematian orang dewasa (15–60 tahun) per 1000 penduduk               |
| 6  | infant deaths                     | Jumlah kematian bayi (<1 tahun) per tahun                                 |
| 7  | Alcohol                           | Konsumsi alkohol per kapita (liter)                                       |
| 8  | percentage expenditure            | Persentase pengeluaran kesehatan terhadap GDP                             |
| 9  | Hepatitis B                       | Persentase imunisasi Hepatitis B pada anak-anak                           |
| 10 | Measles                           | Jumlah kasus campak per tahun                                             |
| 11 | BMI                               | Indeks massa tubuh rata-rata penduduk                                     |
| 12 | under-five deaths                 | Jumlah kematian anak di bawah 5 tahun                                     |
| 13 | Polio                             | Persentase imunisasi polio pada anak                                      |
| 14 | Total expenditure                 | Total pengeluaran pemerintah untuk kesehatan (% dari GDP)                 |
| 15 | Diphtheria                        | Persentase imunisasi difteri pada anak                                    |
| 16 | HIV/AIDS                          | Angka kematian akibat HIV/AIDS per 1000 penduduk                          |
| 17 | GDP                               | Produk Domestik Bruto per kapita                                          |
| 18 | Population                        | Jumlah populasi                                                           |
| 19 | thinness  1-19 years              | Persentase kekurusan usia 10–19 tahun                                     |
| 20 | thinness 5-9 years                | Persentase kekurusan usia 5–9 tahun                                       |
| 21 | Income composition of resources   | Indeks komposisi pendapatan (0–1)                                         |
| 22 | Schooling                         | Rata-rata tahun sekolah penduduk usia 25 tahun ke atas                    |

"""

df.shape

"""Dataset memiliki 2938 baris dan 22 kolom"""

df.describe(include="all")

"""df.describe digunakan untuk mengetahui karakteristik statistik terutama pada data numerik, untuk data kategorikal, hanya beberapa statistik yang dapat dihitung. Dengan describe, kita dapat mengetahui informasi seputar: count, unique, top, freq, mean, std, min, 25% (1st quartile), 50% (2nd quartile), dan 75% (3rd quartile)."""

import seaborn as sns
import matplotlib.pyplot as plt

n_cols = 4
n_rows = len(df.columns) // n_cols + 1
fig, axes = plt.subplots(n_rows, n_cols, figsize=(24, n_rows*4))

axes = axes.flatten()
for i, feature in enumerate(df.columns):
    if df[feature].dtype == 'object':
        sns.countplot(x=feature, data=df, ax=axes[i])
        axes[i].set_title(f'Count Plot of {feature}')
    else:
        sns.histplot(df[feature], kde=True, ax=axes[i])
        axes[i].set_title(f'Histogram of {feature}')

for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""Berdasarkan grafik di atas, sebagian besar data memiliki skewed yang sangat besar."""

import seaborn as sns
import matplotlib.pyplot as plt

n_cols = 4
n_rows = len(df.columns) // n_cols + 1
fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows*4))

num_features = df.select_dtypes(include=['int64', 'float64']).columns
axes = axes.flatten()
for i, feature in enumerate(num_features):
    if df[feature].dtype != 'object':
        sns.boxplot(x=df[feature], ax = axes[i])
        axes[i].set_title(f'Box Plot of {feature}')

for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""Hampir seluruh fitur memilki outlier

## Analisis missing values
"""

missing_values = df.isnull().sum()

missing_values = missing_values[missing_values > 0]

missing_values

"""Fitur yang memiliki missing value memiliki jumlah yang beragam, dari 10 hingga 652 baris.

## Analisis duplicated data
"""

duplicates = df.duplicated().sum()

print("Baris duplikat:")
print(duplicates)

"""Tidak ada data duplikat

## Data Cleaning

### Cleaning Missing Value

Data kita memiliki skewness yang tinggi, karena itu kita menentukan treshold untuk membagi 2 fitur berdasarkan tingkat skew. Kali ini kita akan menggunakan abs(skewness) < 0.5
"""

for feature in num_features:
    skewness = df[feature].skew()

    print(f'Skewness of {feature}: {skewness}')

"""Missing value pada fitur dengan skewness yang tinggi akan diisi dengan nilai median, karena nilai median lebih robust terhadap skewed dan outlier. Sedangkan data lainnya akan diisi dengan nilai mean [Filling missing values with Mean and Median]"""

acceptable_skewness = []
non_acceptable_skewness = []

for feature in num_features:
    skewness = df[feature].skew()
    if abs(skewness) < 0.5:
        acceptable_skewness.append(feature)
    else:
        non_acceptable_skewness.append(feature)

print("Acceptable skewness features:")
print(acceptable_skewness)
print("Non-acceptable skewness features:")
print(non_acceptable_skewness)

from sklearn.impute import SimpleImputer
import numpy as np

for feature in acceptable_skewness:
    if df[feature].isnull().sum() > 0:
        imputer = SimpleImputer(strategy='mean')
        df[feature] = imputer.fit_transform(df[[feature]])
        print(f"Missing values in {feature} have been replaced with the mean.")
    else:
        print(f"No missing values in {feature}.")

for feature in non_acceptable_skewness:
    if df[feature].isnull().sum() > 0:
        imputer = SimpleImputer(strategy='median')
        df[feature] = imputer.fit_transform(df[[feature]])
        print(f"Missing values in {feature} have been replaced with the median.")
    else:
        print(f"No missing values in {feature}.")

missing_values = df.isnull().sum()

missing_values = missing_values[missing_values > 0]

missing_values

"""Missing value berhasil ditangani

Meskipun pada analisis sebelumnya telah menunjukkan banyaknya data outlier, kita tidak melakukan pembersihan outlier karena tujuan kita adalah menganalisis hubungan fitur dengan Life Expectancy. Dan data yang kita miliki menggambarkan kondisi aktual [When should I remove an outlier from my dataset?].

### Drop fitur Country

Karena kita ingin menganalisis Life Expectancy secara global, maka data spesifik country tidak diperlukan, cukup status (developing/developed) dari country tersebut.
"""

df.drop(columns='Country', inplace=True)

"""## Transformation

### Log transformation
Seperti yang bisa dilihat pada grafik histogram sebelumnya, terdapat beberapa fitur yang memiliki skewness sangat tinggi. Karena itu kita menetapkan ambang batas abs(1) untuk dilakukan log transformation. Mengapa menggunakan log transformation? [Log Transformation: Purpose and Interpretation] dengan log transformation, kita bisa mengubah data kita menjadi "normal".
"""

log_features = []

for feature in num_features:
    if abs(df[feature].skew()) > 1:
        df[feature] = np.log1p(df[feature])
        log_features.append(feature)

print("Log-transformed features:")
print(log_features)

for feature in log_features:
    df[feature] = np.log1p(df[feature])
    print(f"Log-transformed {feature}.")

"""Fitur dengan skewness tinggi, telah dilakukan log transformation.

### Standardization

Data kita memiliki banyak sekali outlier, karena itu kita menggunakan standardization [Normalization vs Standardization].
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

for feature in num_features:
    df[feature] = scaler.fit_transform(df[[feature]])
    print(f"Standardized {feature}.")

"""Setelah melakukan berbagai tahapan transformasi, lakukan pengecekan pada dataframe kita."""

df.head()

"""### Label Encoding
Kita perlu melakukan label encoding karena model tidak dapat memahami data kategorikal, kare itu kita perlu mentransformasikannya menjadi angka terlebih dahulu [Label Encoding in Python].
"""

from sklearn.preprocessing import LabelEncoder
df['Status'] = LabelEncoder().fit_transform(df['Status'])  # Developed = 0, Developing = 1

"""## Exploratory Data Analysis
Sebelum masuk pada tahap modeling, kita akan melakukan proses Exploratory Data Analysis terlebih dahulu untuk lebih memahami dataset yang dimiliki.

### Persebaran Data
"""

import seaborn as sns
import matplotlib.pyplot as plt

n_cols = 4
n_rows = len(df.columns) // n_cols + 1
fig, axes = plt.subplots(n_rows, n_cols, figsize=(24, n_rows*4))

axes = axes.flatten()
for i, feature in enumerate(df.columns):
    if df[feature].dtype == 'object':
        sns.countplot(x=feature, data=df, ax=axes[i])
        axes[i].set_title(f'Count Plot of {feature}')
    else:
        sns.histplot(df[feature], kde=True, ax=axes[i])
        axes[i].set_title(f'Histogram of {feature}')

for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""Kita telah melakukan beberapa tahap transformasi, sehingga data yang kita miliki mulai mendekati bells curve. Berikut perbandingannya:

| Sebelum      | Sesudah|
|--------------|--------|
|![alt text](image-7.png)| ![alt text](image-8.png)|

### Korelasi antar fitur
Grafik dibawah ini menggambarkan korelasi setiap fitur numerik
"""

plt.figure(figsize=(15, 10))
sns.heatmap(df[num_features].corr(), annot=True, fmt=".2f", cmap='YlGnBu', square=True)

"""### Histogram life expectancy"""

plt.figure(figsize=(10, 6))
sns.histplot(df['Life expectancy '], bins=50, kde=True, color='skyblue')
plt.title('Histogram of Life Expectancy')
plt.xlabel('Life Expectancy')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""histogram dari harapan hidup yang telah distandardisasi Sebagian besar nilai harapan hidup berada di sekitar rata-rata (nilai 0), dengan distribusi yang sedikit condong ke kiri (negatif), menunjukkan bahwa ada lebih banyak negara dengan harapan hidup di bawah rata-rata dibandingkan yang di atas. Pola ini mengindikasikan ketimpangan dalam distribusi harapan hidup antar negara secara global.

### Life Expectancy based on Countries Status
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(4, 6))
sns.violinplot(x='Status', y='Life expectancy ', data=df, inner='box', palette='dark', hue='Status')
plt.title('Life expectancy Based on Countries status')
plt.grid(True, alpha=0.3)
plt.show()

"""violin plot diatas yang menunjukkan distribusi harapan hidup berdasarkan status negara, di mana status 0 dan 1 mewakili negara maju (developed) dan negara berkembang (developing). Terlihat bahwa negara dengan status 0 memiliki distribusi harapan hidup yang lebih tinggi dan merata dibandingkan status 1, yang memiliki sebaran lebih luas dan mencakup nilai harapan hidup yang sangat rendah. Artinya, negara dengan status 0 cenderung memiliki harapan hidup yang lebih baik secara konsisten.

### Korelasi fitur numerik dengan Life Expectancy
"""

var_corr = df[num_features]

var_corr

"""var_corr menampilkan nilai korelasi antar setiap fitur numerik. Selanjutnya, kita ingin melakukan analisis antara fitur numerik dengan target corr (Life expectancy) dengan tahapan seperti di bawah ini."""

target_corr = df['Life expectancy ']

var_corr = df[num_features]
var_corr = var_corr.drop(columns=['Life expectancy '])


print(var_corr)

var_corr

df

correlation = var_corr.corrwith(target_corr)


correlation_sorted = correlation.abs().sort_values(ascending=False)

plt.figure(figsize=(10,6))

correlation_sorted.plot(kind='bar')
plt.title(f'Correlation with Life expectancy')
plt.xlabel('Variables')
plt.ylabel('Correlation coefficient')
plt.show()

"""Berdasarkan bar plot di atas, dapat dilihat 5 fitur yang paling berpengaruh dengan life expectancy adalah:
 - HIV/AIDS
 - Schooling
 - under-fixe deaths
 - Infant deaths
 - Income composition of resources

### Korelasi fitur kategorikal dengan Life Expectancy

Setelah menganalisis korelasi antara fitur numerik, berikutnya proses analisis korelasi dilakukan dengan fitur kategorikal.

Fitur categorical yang kita gunakan adalah Status (Developed dan Developing)
"""

df[['Life expectancy ', 'Status']].corr()

"""Dapat dilihat korelasi fitur kategorikal dan fitur target adalah -0.48141473517841626.

### Feature selection

Sebelum membangun model, kita memilih fitur penting supaya model dapat berfokus pada fitur yang benar-benar berpengaruh [Why Feature Selection is Critical for Machine Learning].
"""

correlation_matrix = df.corr()

print(correlation_matrix)

important_features = []

for feature in correlation_matrix.columns:
    if abs(correlation_matrix['Life expectancy '][feature]) > 0.5 and feature != 'Life expectancy ':
        important_features.append(feature)
        print(f"Feature '{feature}' has a strong correlation with 'Life expectancy ': {correlation_matrix['Life expectancy '][feature]}\n")
    else:
        print(f"Feature '{feature}' does not have a strong correlation with 'Life expectancy ': {correlation_matrix['Life expectancy '][feature]}\n")

print("Important features:")
print(important_features)

"""Importan features yang akan kita gunakan dalam pembangunan model adalah:
['infant deaths', ' BMI ', 'under-five deaths ', ' HIV/AIDS', 'GDP', ' thinness  1-19 years', ' thinness 5-9 years', 'Income composition of resources', 'Schooling']
"""

df_important = df[important_features + ['Life expectancy ']]

df_important

"""variabel df_important merupakan variabel yang menyimpan important feature dengan target variabel.

### Data splitting
Pertama, kita perlu membagi dua dataset yang kita miliki menjadi data train dan data test.
Langkah-langkah untuk melakukan data splitting adalah:
 - Inisiasi variabel:
    - X : variabel-variabel fitur selain target
    - y : variabel target
 - Memanggil fungsi train test split, hasil data splitting akan disimpan pada variabel : X_train, X_test, y_train, y_test
 - Parameter yang digunakan yaitu:
   - X, y : data yang ingin dibagi
   - test_size : ukuran test size yang digunakan, pada kasus ini menggunakan 0.2 (80:20)
   - random_state=42 : memastikan konsistensi data yang dihasilkan meskipu proses data splitting dilakukan secara berulang.
"""

from sklearn.model_selection import train_test_split

X = df_important.drop(columns='Life expectancy ')
y = df_important['Life expectancy ']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Jumlah data: ",len(X))

print("Jumlah data latih: ",len(X_train))

print("Jumlah data test: ",len(X_test))

"""## Modeling
Setelah mempersiapkan dataset, kita dapat mulai membangun model. Ada beberapa tahapan yang perlu dilakukan, seperti yang akan dijelaskan dibawah ini.

Setelah melakukan data splitting, kemudian kita melakukan pengecekan jumlah data, seperti dibawah ini:

### Model initialization
Langkah berikutnya adalah inisiasi model.
"""

from sklearn.linear_model import LinearRegression, Ridge
from sklearn.linear_model import Lars
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.metrics import mean_squared_error

models = {
    "Linear Regression": LinearRegression(),
    "LARS": Lars(),
    "Gradient Boosting": GradientBoostingRegressor(),
    "Random Forest": RandomForestRegressor(),
    "Ridge Regression": Ridge()
}

"""Penjelasan model yang digunakan adalah:

| Nama Model    | Kelebihan & Kekurangan     | Referensi  |
|---------------|---------------|-------------|
| Linear Regression  | Linear Regression menawarkan kesederhanaan dan interpretabilitas tinggi, tetapi terbatas hanya untuk hubungan linier dan sensitif terhadap outlier. | [Linear Regression in Machine learning]|
| LARS    | Least Angle Regression (LARS) unggul dalam dataset berdimensi tinggi dan efektif untuk seleksi fitur, namun lebih kompleks untuk diinterpretasi dan kurang populer. | [Least Angle Regression (LARS)]|
|Gradient Boosting  | Gradient Boosting memberikan performa sangat tinggi untuk hubungan kompleks, tetapi membutuhkan tuning parameter yang ekstensif dan komputasi berat.  | [Gradient Boosting] |
| Random Forest | Random Forest menawarkan keseimbangan yang baik dengan performa solid tanpa banyak penyesuaian dan ketahanan terhadap overfitting, meskipun kurang interpretable. | [Random Forest Algorithm in Machine Learning] |
| Ridge Regression  | Ridge Regression efektif mengatasi multikolinearitas melalui regularisasi L2, tetapi tidak melakukan seleksi fitur dan masih terbatas pada hubungan linier. | [Ridge Regression] |

Meskipun kita telah mengetahui karakteristik dari masing-masing algoritma, kita perlu mengujinya terlebih dahulu, karena dataset itu unik, sehingga tidak ada algoritma yang sejatinya langsung sesuai dengan algoritma tertentu. Untuk itu, kita akan melakukan pelatihan dan evaluasi untuk memilih algoritma.

### Evaluation

Untuk melakukan tahap evaluasi, berikut ini langkah-langkah yang dapat dijalankan:

### Membuat dataframe
EvaluasiDataframe mencakup nama model, serta metrik yang digunakan. Pada kasus ini, kita menggunakan metrik MSE, RMSE, R2.
"""

df_results = pd.DataFrame(columns=["Model", "MSE", "RMSE", "R2"])

"""Dilansir dari [Metrics for Linear Regression Effectiveness: R-squared, MSE and RSE] penjelasan dari masing-masing metrik adalah sebagai berikut:

| Metrik  | Deskripsi       | Cara kerja      | Formula |
|---------|-----------------|-----------------|--------|
| Mean Squared Error (MSE)  | MSE adalah metrik yang mengukur rata-rata kuadrat dari error (selisih antara nilai prediksi dan nilai aktual). Semakin kecil nilai MSE, semakin baik model prediksi.  | MSE bekerja dengan menghitung selisih antara nilai prediksi dan nilai aktual, mengkuadratkan selisih tersebut (untuk menghilangkan nilai negatif), kemudian menghitung nilai rata-ratanya. Pengkuadratan membuat MSE lebih sensitif terhadap outlier. | $MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$|
| Root Mean Squared Error (RMSE)  | RMSE adalah akar kuadrat dari MSE, digunakan untuk mendapatkan error dalam unit yang sama dengan variabel yang diprediksi. RMSE lebih mudah diinterpretasi karena berada dalam skala data asli. | RMSE bekerja dengan menghitung MSE terlebih dahulu, kemudian mengambil akar kuadratnya. Seperti MSE, RMSE juga sensitif terhadap outlier dan nilai yang lebih besar memberikan penalti yang lebih besar.  | $RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$ |
|Coefficient of Determination (R²)  | R² mengukur proporsi variasi dalam variabel dependen yang dapat dijelaskan oleh variabel independen dalam model. Nilainya berkisar antara 0 hingga 1, dimana nilai yang lebih tinggi menunjukkan model yang lebih baik. | R² bekerja dengan membandingkan error model dengan variasi total data. R² mengukur seberapa baik model menangkap pola dalam data dibandingkan dengan hanya menggunakan nilai rata-rata sebagai prediksi.  | $R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$

Setelah menyiapkan dataframe dan metrik evaluasi, selanjutkan lakukan traininng dan evaluasi pada model yang telah dilakukan inisiasi.
"""

results = []

for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = mse ** 0.5
    r2 = model.score(X_test, y_test)

    results.append({
        "Model": model_name,
        "MSE": mse,
        "RMSE": rmse,
        "R2": r2
    })

df_results = pd.DataFrame(results)

print(df_results)

"""Dari hasil evaluasi tersebut, algoritma menunjukkan keunggulan dengan MSE dan RMSE terendah, serta R2 tertinggi. Sehingga algoritma yang dipilih adalah **random forest.**

### Visualisasi prediksi model
"""

rf_model = models['Random Forest'].fit(X_train, y_train)

hiv_column_name = X_test.columns[3]

sorted_indices = X_test[hiv_column_name].sort_values().index
X_test_sorted = X_test.loc[sorted_indices]
y_test_sorted = y_test.loc[sorted_indices]

y_pred_sorted = rf_model.predict(X_test_sorted)

plt.figure(figsize=(8, 6))
plt.scatter(X_test[hiv_column_name], y_test, color='red', alpha=0.7, s=30)
plt.plot(X_test_sorted[hiv_column_name], y_pred_sorted, color='blue', linewidth=2)
plt.xlabel('HIV/AIDS')
plt.ylabel('Life Expectancy')
plt.title('Random Forest: Actual vs Predicted Values')
plt.legend(['Predicted', 'Actual'])
plt.tight_layout()
plt.show()

"""Algoritma random forest telah melakukan prediksi dengan baik.

### Hyperparameter tuning

Meskipun hasil telah memuaskan, kita belum melakukan tuning apapun terhadap model kita. Karena itu, kita dapat melakukan penyesuaian model melalui hyperparameter tuning.Metode yang digunakan adalah Bayesian search optimization.
"""

import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from skopt import BayesSearchCV
from skopt.space import Integer, Real, Categorical

search_space = {
    'n_estimators': Integer(50, 300),
    'max_depth': Integer(5, 30),
    'min_samples_split': Integer(2, 20),
    'min_samples_leaf': Integer(1, 10),
    'max_features': Categorical(['sqrt', 'log2', None]),
    'bootstrap': Categorical([True, False])
}

rf_model = RandomForestRegressor(random_state=42)

bayes_search = BayesSearchCV(
    estimator=rf_model,
    search_spaces=search_space,
    n_iter=50,
    cv=5,
    n_jobs=-1,
    verbose=1,
    scoring='neg_mean_squared_error',
    random_state=42
)

bayes_search.fit(X_train, y_train)

best_params = bayes_search.best_params_
best_rf = bayes_search.best_estimator_

y_pred = best_rf.predict(X_test)
tuned_mse = mean_squared_error(y_test, y_pred)
tuned_rmse = np.sqrt(tuned_mse)
tuned_r2 = r2_score(y_test, y_pred)

print("Best hyperparameters:", best_params)
print("\nOptimized Random Forest Performance:")
print(f"MSE: {tuned_mse:.6f}")
print(f"RMSE: {tuned_rmse:.6f}")
print(f"R²: {tuned_r2:.6f}")
print("\nImprovement:")
print(f"Original R²: {0.959501:.6f}")
print(f"Tuned R²: {tuned_r2:.6f}")
print(f"R² Improvement: {tuned_r2 - 0.959501:.6f}")

"""Outputnya adalah sebagai berikut:
Best hyperparameters: OrderedDict({'bootstrap': False, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 77})

Optimized Random Forest Performance:
MSE: 0.029405
RMSE: 0.171480
R²: 0.969331

Improvement:
Original R²: 0.959501
Tuned R²: 0.969331
R² Improvement: 0.009830

### Visualisasi tuned model

Berikut ini visualisasi prediksi dengan tuned model. Tidak terlihat perbedaan yang signifikan karena evaluasi sebelumnya telah memuaskan.
"""

if isinstance(X_test, pd.DataFrame):
    hiv_column = X_test.iloc[:, 3]
    sorted_indices = hiv_column.sort_values().index
    X_test_sorted = X_test.loc[sorted_indices]
    y_test_sorted = y_test.loc[sorted_indices]
else:
    sorted_indices = np.argsort(X_test[:, 3])
    X_test_sorted = X_test[sorted_indices]
    y_test_sorted = y_test[sorted_indices]

y_pred_sorted = best_rf.predict(X_test_sorted)

plt.figure(figsize=(10, 6))
if isinstance(X_test, pd.DataFrame):
    plt.scatter(hiv_column, y_test, color='red', alpha=0.7, s=30)
    plt.plot(X_test_sorted.iloc[:, 3], y_pred_sorted, color='blue', linewidth=2)
else:
    plt.scatter(X_test[:, 3], y_test, color='red', alpha=0.7, s=30)
    plt.plot(X_test_sorted[:, 3], y_pred_sorted, color='blue', linewidth=2)

plt.xlabel('HIV/AIDS')
plt.ylabel('Life Expectancy')
plt.title('Tuned Random Forest: Actual vs Predicted Life Expectancy')
plt.legend(['Predicted', 'Actual'])
plt.tight_layout()
plt.show()

"""### Evaluasi tuned model"""

import pandas as pd

original_rf = {
    "Model": "Random Forest (Original)",
    "MSE": 0.038830,
    "RMSE": 0.197053,
    "R2": 0.959501
}

tuned_rf = {
    "Model": "Random Forest (Tuned)",
    "MSE": 0.029405,
    "RMSE": 0.171480,
    "R2": 0.969331
}

rf_comparison = pd.DataFrame([original_rf, tuned_rf])

pd.options.display.float_format = '{:.6f}'.format

print("Random Forest Model Comparison:")
print(rf_comparison)

"""Dataframe diatas menampilkan perbandingan evaluasi model sebelum dan sesudah dilakukan proses tuning dengan metrik yang telah ditentukan.

Tuned model berhasil menurunkan MSE dan RMSE, serta meningkatkan R2.

"""

mse_improvement = (original_rf["MSE"] - tuned_rf["MSE"]) / original_rf["MSE"] * 100
rmse_improvement = (original_rf["RMSE"] - tuned_rf["RMSE"]) / original_rf["RMSE"] * 100
r2_improvement = (tuned_rf["R2"] - original_rf["R2"]) / original_rf["R2"] * 100

print("\nImprovement Metrics:")
print(f"MSE: Reduced by {mse_improvement:.2f}%")
print(f"RMSE: Reduced by {rmse_improvement:.2f}%")
print(f"R2: Increased by {r2_improvement:.2f}%")

print("\nOptimal Hyperparameters:")
print("n_estimators: 77")
print("max_depth: 20")
print("min_samples_split: 2")
print("min_samples_leaf: 1")
print("max_features: 'sqrt'")
print("bootstrap: False")

plt.figure(figsize=(12, 6))

bar_width = 0.35
r1 = np.arange(3)
r2 = [x + bar_width for x in r1]

metrics = ["MSE", "RMSE", "R²"]
original_values = [original_rf["MSE"], original_rf["RMSE"], original_rf["R2"]]
tuned_values = [tuned_rf["MSE"], tuned_rf["RMSE"], tuned_rf["R2"]]

plt.bar(r1, original_values, width=bar_width, color='blue', alpha=0.7, label='Original Random Forest')
plt.bar(r2, tuned_values, width=bar_width, color='green', alpha=0.7, label='Tuned Random Forest')

plt.xlabel('Metrics')
plt.ylabel('Values')
plt.title('Random Forest: Original vs Tuned Performance')
plt.xticks([r + bar_width/2 for r in range(3)], metrics)
plt.legend()

for i, v in enumerate(original_values):
    plt.text(i - 0.05, v + 0.01, f"{v:.4f}", ha='center', fontsize=9)

for i, v in enumerate(tuned_values):
    plt.text(i + bar_width - 0.05, v + 0.01, f"{v:.4f}", ha='center', fontsize=9)

plt.tight_layout()
plt.show()

"""Grafik diatas menampilkan tuned model berhasil menurunkan MSE dan RMSE, serta meningkatkan R2.

"""



"""[//]:#

[Sejawat]: <https://sejawat.co.id/article/detail/life-expectancy-memperpanjang-harapan-rentang-hidup-1680701940>

[Erupean Commission]: <https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Glossary:Life_expectancy>

[Determinants in Predicting Life Expectancy Using Machine Learning]: <https://doi.org/10.23947/2687-1653-2022-22-4-373-383>

[Life Expectancy (WHO)]: <https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who>

[Filling missing values with Mean and Median]: <https://tahera-firdose.medium.com/filling-missing-values-with-mean-and-median-76635d55c1bc>

[When should I remove an outlier from my dataset?]: <https://www.scribbr.com/frequently-asked-questions/when-to-remove-an-outlier/>

[Log Transformation: Purpose and Interpretation]: <https://medium.com/@kyawsawhtoon/log-transformation-purpose-and-interpretation-9444b4b049c9>

[Normalization vs Standardization]: <https://www.geeksforgeeks.org/normalization-vs-standardization/>

[Why Feature Selection is Critical for Machine Learning]: <https://ujangriswanto08.medium.com/why-feature-selection-is-critical-for-machine-learning-3913fffd62c0>

[ML | Handling Missing Values]: <https://www.geeksforgeeks.org/ml-handling-missing-values/>

[Label Encoding in Python]: <https://www.geeksforgeeks.org/ml-label-encoding-of-datasets-in-python/>

[Linear Regression in Machine learning]: <https://www.geeksforgeeks.org/ml-linear-regression/>

[Least Angle Regression (LARS)]: <https://www.geeksforgeeks.org/least-angle-regression-lars/>

[Gradient Boosting]: <https://www.geeksforgeeks.org/ml-gradient-boosting/>

[Random Forest Algorithm in Machine Learning]: <https://www.geeksforgeeks.org/random-forest-algorithm-in-machine-learning/>

[Ridge Regression]: <https://www.geeksforgeeks.org/what-is-ridge-regression/>

[Metrics for Linear Regression Effectiveness: R-squared, MSE and RSE]: <https://learn.saylor.org/mod/page/view.php?id=80811>
"""